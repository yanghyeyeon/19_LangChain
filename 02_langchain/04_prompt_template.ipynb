{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-4o-mini\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from_template() ë©”ì†Œë“œ\n",
    "* ë³€ìˆ˜ë¥¼ ì¤‘ê´„í˜¸ë¡œ ë¬¶ì–´ì„œ í…œí”Œë¦¿ì— ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name'] input_types={} partial_variables={} template='{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template ì •ì˜, {}ì•ˆì˜ ë‚´ìš©ì€ ì´í›„ì— ê°’ì´ ë“¤ì–´ê°ˆ ìë¦¬\n",
    "template = \"{name}ì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "# from_template ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ PromptTemplate ê°ì²´ë¥¼ ìƒì„±\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bearì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "# prompt ìƒì„± (ì™„ì„±) format() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ë³€ìˆ˜ì— ê°’ì„ ë„£ìŒ\n",
    "prompt = prompt.format(name=\"bear\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['langauge'] input_types={} partial_variables={} template='{langauge}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"{langauge}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì„¬(Guido van Rossum)ì— ì˜í•´ 1991ë…„ì— ì²˜ìŒ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ì´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ë§Œë“¤ ë•Œ ì½”ë“œì˜ ê°€ë…ì„±ê³¼ ì‚¬ìš©ì˜ ìš©ì´ì„±ì„ ì¤‘ì‹œí•˜ì˜€ìœ¼ë©°, Pythonì€ ì´í›„ë¡œë„ ë§ì€ ê°œë°œìë“¤ ì‚¬ì´ì—ì„œ ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ë³¸ì ìœ¼ë¡œëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€ìˆ˜ë¥¼ ì‘ì„±í•´ì•¼ í•˜ì§€ë§Œ\n",
    "# ë³€ìˆ˜ê°€ í•œê°œì¼ë•ŒëŠ” ë³€ìˆ˜ë§Œìœ¼ë¡œ ì‘ì„±ì´ ê°€ëŠ¥\n",
    "\n",
    "# print(chain.invoke({\"langauge\" : \"Python\"}).content)\n",
    "print(chain.invoke({\"Python\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate ê°ì²´ ìƒì„±ê³¼ ë„ì‡ì— prompt ìƒì„±\n",
    "* input_variables ì¸ìë¥¼ ì‚¬ìš©í•´ ë³€ìˆ˜ë¥¼ ì§€ì •í•œë‹¤.\n",
    "* í…œí”Œë¦¿ì— ì‘ì„±í•œ ë³€ìˆ˜ê°€ input_variablesì— ì—†ìœ¼ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œì¤€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PythonëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language}ëŠ” ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language\"],\n",
    "    \n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial_variables**\n",
    "* ì—°ì‚°ì¤‘ ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ ë„£ì„ ìˆ˜ ìˆë‹¤.\n",
    "* í•­ìƒ ê³µí†µëœ ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê³  ì‹¶ì€ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‚¬ìš©<br>\n",
    "ex) ë‚ ì§œ, ì‹œê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'Java'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "# template ì •ì˜\n",
    "template = \"{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "# PromptTemplate ê°ì²´ ìƒì„±\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"language1\"],\n",
    "    partial_variables={\n",
    "        \"language2\" : \"Java\" # dictionary í˜•íƒœë¡œ partial_variables ì‘ì„±\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythonê³¼ JavaëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language1=\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**partial()**\n",
    "* ì—°ì‚°ì¤‘ì— ë¯¸ë¦¬ ê³„ì‚°ëœ ë³€ìˆ˜ë¥¼ í””ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§€ì •í•´ì„œ ë„£ì„ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language1'] input_types={} partial_variables={'language2': 'JavaScript'} template='{language1}ê³¼ {language2}ëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?'\n"
     ]
    }
   ],
   "source": [
    "prompt_partial = prompt.partial(language2=\"JavaScript\")\n",
    "\n",
    "print(prompt_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Pythonê³¼ JavaScriptëŠ” ê°ê° ëˆ„ê°€ ë§Œë“¤ì—ˆë‚˜ìš”?')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_partial.invoke(\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pythonì€ ê·€ë„ ë°˜ ë¡œì¸(Guido van Rossum)ì— ì˜í•´ 1991ë…„ì— ì²˜ìŒ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” Pythonì„ ë§Œë“¤ ë•Œ ì½”ë“œì˜ ê°€ë…ì„±ì„ ì¤‘ìš”ì‹œí•˜ë©°, í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ì‚¬ìš©ì„ ì‰½ê²Œ í•˜ë ¤ëŠ” ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "JavaScriptëŠ” ë¸Œë Œë˜ ì•„ì´í¬(Brendan Eich)ì— ì˜í•´ 1995ë…„ì— ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ë‹¹ì‹œ ë„·ìŠ¤ì¼€ì´í”„(Netscape)ì—ì„œ ì¼í•˜ê³  ìˆì—ˆìœ¼ë©°, ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ë™ì ì¸ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê¸° ìœ„í•´ JavaScriptë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. JavaScriptëŠ” ì²˜ìŒì—ëŠ” \"Mocha\"ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë¶ˆë ¸ê³ , ì´í›„ \"LiveScript\"ë¥¼ ê±°ì³ í˜„ì¬ì˜ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_partial | llm\n",
    "\n",
    "print(chain.invoke(\"Python\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŒŒì¼ë¡œ ë¶€í„° template ì½ì–´ì˜¤ê¸°\n",
    "* promptë¥¼ í¸í•˜ê²Œ ì‘ì„± ë° ìˆ˜ì •\n",
    "* ìƒí™©ì— ë§ê²Œ íŒŒì¼ì„ ì‘ì„±í•´ë‘ë©´ í•„ìš”í• ë•Œë°”ë‹¤ êº¼ë‚´ì„œ ì“¸ìˆ˜ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} template='{language} ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"prompts/language_simple.yaml\", encoding=\"utf-8\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python ì–¸ì–´ì— ëŒ€í•´ 3ì¤„ë¡œ ì„¤ëª…í•´ì¤˜.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(language=\"Python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ì–¸ì–´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
      "ì–¸ì–´ì˜ íŠ¹ì§•ì„ ë‹¤ìŒì˜ ì–‘ì‹ì— ë§ê²Œ ì •ë¦¬í•˜ì„¸ìš”\n",
      "300ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
      "í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
      "---\n",
      "#ì–‘ì‹\n",
      "1. íŠ¹ì§•\n",
      "2. ì œì‘ì\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = load_prompt(\"prompts/language.yaml\", encoding=\"utf-8\")\n",
    "\n",
    "print(prompt2.format(language=\"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. íŠ¹ì§•: JavaëŠ” ê°ì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ, í”Œë«í¼ ë…ë¦½ì„±ì„ ê°•ì¡°í•©ë‹ˆë‹¤. \"Write Once, Run Anywhere\"ë¼ëŠ” ìŠ¬ë¡œê±´ ì•„ë˜, JVM(Java Virtual Machine)ì„ í†µí•´ ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê°•ë ¥í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ìë™ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ APIë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. ì œì‘ì: JavaëŠ” 1995ë…„ ì¬ ë§ˆì´í¬ë¡œì‹œìŠ¤í…œì¦ˆ(Sun Microsystems)ì˜ ì œì„ìŠ¤ ê³ ìŠ¬ë§(James Gosling)ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. ëŒ€í‘œì ì¸ í”„ë ˆì„ì›Œí¬: Spring, Hibernate, JavaServer Faces(JSF), Apache Struts ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì•¼: JavaëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜, ëª¨ë°”ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜(íŠ¹íˆ ì•ˆë“œë¡œì´ë“œ), ê¸°ì—…ìš© ì†Œí”„íŠ¸ì›¨ì–´, ë¹…ë°ì´í„° ì²˜ë¦¬ ë° í´ë¼ìš°ë“œ ì»´í“¨íŒ… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt2 | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0) | StrOutputParser()\n",
    "\n",
    "answer = chain.invoke(\"Java\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPrompTemplate\n",
    "* ëŒ€í™” ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì…í•˜ê³ ì í•  ë•Œ ì‚¬ìš© í•  ìˆ˜ ìˆë‹¤.\n",
    "* ë©”ì‹œì§€ëŠ” íŠœí”Œí˜•íƒœë¡œ ì „ë‹¬\n",
    "    * (\"role\", \"message\") êµ¬ì„±ë˜ê³ , ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ìƒì„± ê°€ëŠ¥\n",
    "\n",
    "**role**\n",
    "* system : ì‹œìŠ¤í…œ ì„¤ì •ë©”ì‹œì§€ë¡œ ì£¼ë¡œ ì „ì—­ì„œì •ì„ í• ë•Œ ì‚¬ìš©\n",
    "* human : ì‚¬ìš©ìì˜ ì…ë ¥ ë©”ì‹œì§€\n",
    "* ai : AIì˜ ë‹µë³€ ë©”ì‹œì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['language'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\"{language}ì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: pythonì˜ ì œì‘ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt.format(language=\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì°Œ ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë°˜ê°€ì›Œìš”!', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}), HumanMessage(content='ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆë‹¤?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # role, message\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì€ {name} ì…ë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"ë°˜ê°€ì›Œìš”!\"),\n",
    "        (\"ai\", \"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    name=\"ë¬´ì°Œ\", user_input=\"ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì…ë‹ˆë‹¤?\"\n",
    ")\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì œ ì´ë¦„ì€ ë¬´ì°Œì…ë‹ˆë‹¤! ë‹¹ì‹ ê³¼ ëŒ€í™”í•˜ê²Œ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤. ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆŒê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagePlaceHolder\n",
    "* ì•„ì§ í™•ì •ëœ ë©”ì‹œì§€ê°€ ì•„ë‹ˆì§€ë§Œ, ë‚˜ì¤‘ì— ì±„ì›Œì§ˆ ë©”ì‹œì§€ ìœ„ì¹˜ë¥¼ ì¡ì•„ë‘ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤.\n",
    "* ë³´í†µ ëŒ€í™” ê¸°ë¡ì„ í•˜ê³ ì‹¶ì„ë•Œ ì‚¬ìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['conversation', 'word_count'] input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EAE22CBC40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count} ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "        (\"human\", \"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ {word_count} ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: ë‹¹ì‹ ì€ ìš”ì•½ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ìš” í‚¤ì›Œë“œë¡œ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
      "Human: íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\n",
      "AI: ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\n",
      "Human: set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\n",
      "AI: ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\n",
      "Human: ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë¥¼ 5 ë‹¨ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count=5,\n",
    "    conversation=[\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'íŒŒì´ì¬, ë¦¬ìŠ¤íŠ¸, ì¤‘ë³µ ì œê±°, set.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"word_count\":5,\n",
    "    \"conversation\":[\n",
    "        (\"human\", \"íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ìˆ«ìë¥¼ ì œê±°í•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?\"),\n",
    "        (\"ai\", \"ì•„ì£¼ ê°„ë‹¨í•´ìš”! set()ì„ ì‚¬ìš©í•˜ë©´ ì¤‘ë³µì„ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹µë‹ˆë‹¤.\"),\n",
    "        (\"human\", \"set()ì´ ë­”ì§€ëŠ” ì˜ ëª¨ë¥´ê² ì§€ë§Œ ì—„ì²­ ê°„ë‹¨í•˜ë„¤! ì´ê±¸ë¡œ í•´ê²°ëì–´ ã…ã…\"),\n",
    "        (\"ai\", \"ë„¤, setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ìë£Œí˜•ì´ì—ìš”! ì •ë§ ìœ ìš©í•˜ì£ . ì•ìœ¼ë¡œë„ íŒŒì´ì¬ì˜ í¸ë¦¬í•œ ê¸°ëŠ¥ë“¤ì„ ë§ì´ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆì–´ìš”~ ğŸ˜Š\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ì•½ 24ì‹œê°„ì…ë‹ˆë‹¤. ì¦‰, ì§€êµ¬ê°€ í•œ ë°”í€´ ëŒì•„ì„œ í•˜ëŠ˜ì„ ë‹¤ì‹œ ë³¸ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê¸°ê°„ì€ \"í•˜ë£¨\"ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤.\\n\\n\\ní•˜ì§€ë§Œ ì‹¤ì œë¡œ ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ì •í™•íˆ 23ì‹œê°„ 56ë¶„ 4ì´ˆë¼ëŠ” ë” ì •êµí•œ ê°’ì…ë‹ˆë‹¤. ì´ ì°¨ì´ëŠ” íƒœì–‘ê´‘ì´ ì§€êµ¬ë¥¼ ê°ìŒ€ ë•Œê¹Œì§€ ì§€êµ¬ê°€ íšŒì „í•˜ëŠ” ì‹œê°„ê³¼ íƒœì–‘ì„ ê´€ì°°í•  ë•Œ ë‚˜íƒ€ë‚˜ëŠ” ì‹œê°„ ì‚¬ì´ì˜ ì°¨ì´ ë•Œë¬¸ì…ë‹ˆë‹¤.\\n\\n* **ì‹œì°¨**: ê° ì§€ì—­ì€ í•´ë‹¹ ê¸°ì¤€ ì‹œê°ì— ë”°ë¥¸ ë³¸ë˜ ìì „ ì£¼ê¸°ë¥¼ ë”°ë¼ì„œ 24ì‹œê°„ìœ¼ë¡œ ì„¤ì •ëœ í•˜ë£¨ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì§„ì •í•œ ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” íƒœì–‘ê³¼ì˜ ìƒí˜¸ ì‘ìš©ì„ í†µí•´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# model\n",
    "llm = Ollama(model=\"gemma2:9b\")\n",
    "\n",
    "response = llm.invoke(\"ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ”?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20118\\AppData\\Local\\Temp\\ipykernel_17464\\2870157151.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "model = ChatOllama(\n",
    "    model = \"gemma2:9b\",\n",
    "    temperature=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'íŒŒì´ì¬, ì¤‘ë³µ ì œê±°, set ì‚¬ìš©, ê°„ë‹¨ í•´ê²°, ìœ ìš©  \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
